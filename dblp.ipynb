{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLo9jjs2Iv0H"
   },
   "source": [
    "[Dataset](https://www.aminer.org/citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGX0uzun0lpC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def preprocessing(filename = \"dblpv13.json\"):\n",
    "  '''\n",
    "    將NumberInt(N)置換成整數N\n",
    "\n",
    "    TODO: 111/9/6 bigjson替換成ijson\n",
    "  '''\n",
    "\n",
    "  with open(DIR_PATH + filename, \"r\") as f:\n",
    "    with open(DIR_PATH + \"output\", \"w\") as outputFile:\n",
    "      while True:\n",
    "        content = f.readline()\n",
    "        if content == \"\": \n",
    "          break\n",
    "\n",
    "        erasePosStart = content.find(\"NumberInt(\")\n",
    "        if erasePosStart != -1:\n",
    "          erasePosEnd = content.find(\")\", erasePosStart)\n",
    "          content = content[:erasePosStart] + \\\n",
    "                content[erasePosStart+10 : erasePosEnd] + \\\n",
    "                content[erasePosEnd+1:]\n",
    "\n",
    "        outputFile.write(content)\n",
    "  try:\n",
    "    f = open(DIR_PATH + \"output\", \"rb\")\n",
    "    jsonFormat = bigjson.load(f)\n",
    "    size = len(jsonFormat)\n",
    "  except Exception as e:\n",
    "    print(str(e))\n",
    "    return\n",
    "  else:\n",
    "    os.remove(DIR_PATH + filename)\n",
    "    os.rename(DIR_PATH + \"output\", DIR_PATH + filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njKqns0f5Zqj"
   },
   "outputs": [],
   "source": [
    "import ijson\n",
    "from os.path import exists\n",
    "\n",
    "def captureNodes():\n",
    "    '''\n",
    "    從原始資料集中擷取authers的id作為節點，並且排除掉重複後輸出至node\n",
    "    '''\n",
    "    nodes = set()\n",
    "    with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "        authors = ijson.items(f, \"item\")\n",
    "        author = \"\"\n",
    "        try:\n",
    "            for author in authors:\n",
    "                if \"_id\" in author:\n",
    "                    nodes.add(author[\"_id\"])\n",
    "        except Exception as e:\n",
    "            print(author)\n",
    "          \n",
    "    with open(DIR_PATH + \"nodes\", \"w\") as output:\n",
    "        for node in nodes:\n",
    "            output.write(\"\\\"\" + node + \"\\\"\\n\")\n",
    "\n",
    "def captureEdges():\n",
    "    '''\n",
    "    若任兩個節點只要有一篇共同著作，則視為有邊連在一起\n",
    "    '''\n",
    "    output = open(DIR_PATH + \"edges\", \"w\")\n",
    "    with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "        parser = ijson.parse(f)\n",
    "        for prefix, event, value in parser:\n",
    "            if prefix == \"item.authors\" and event == \"start_array\":\n",
    "                authorsInSamePaper = set()\n",
    "                while event != \"end_array\":\n",
    "                    prefix, event, value = next(parser)\n",
    "                    if prefix == \"item.authors.item._id\":\n",
    "                        authorsInSamePaper.add(value)\n",
    "                        \n",
    "\n",
    "        for c in combinations(authorsInSamePaper, 2):\n",
    "            output.write(c[0] + \",\" + c[1] + \"\\n\")\n",
    "\n",
    "def extractTopic():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15561,
     "status": "ok",
     "timestamp": 1665457440579,
     "user": {
      "displayName": "HeJi Sky",
      "userId": "17996262020646699163"
     },
     "user_tz": -480
    },
    "id": "8pZsK6w6Z5aM",
    "outputId": "c8fd8f63-1fb8-4c0a-860a-316ba3f9c56a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DIR_PATH = \"\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/dirve\")\n",
    "    DIR_PATH = r\"/content/dirve/MyDrive/研究所/Data/dblp/\"\n",
    "    sys.path.append('/content/dirve/MyDrive/Colab Notebooks/package')\n",
    "else:\n",
    "    DIR_PATH = r\"D:\\\\論文實驗\\\\data\\\\dblp\\\\\"\n",
    "    sys.path.append('D:\\\\論文實驗\\\\package')\n",
    "    sys.path.append(\"D:\\\\論文實驗\\\\env\\\\Lib\\\\site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1664524239200,
     "user": {
      "displayName": "HeJi Sky",
      "userId": "17996262020646699163"
     },
     "user_tz": -480
    },
    "id": "PY3-d9YdvJyV",
    "outputId": "35271272-7498-4779-b013-bf02c81f069e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.006s\n",
      "\n",
      "OK\n",
      "DEBUG: Allocate 1 to 0\n",
      "INFO: Allocation is complete.\n",
      "INFO: user 0 choose main itemset.\n",
      "DEBUG: itemset: 1\n",
      "INFO: user: 0, traded items:1\n",
      "INFO: 0 tries to activate 1: False\n",
      "INFO: 0 tries to activate 2: True\n",
      "DEBUG: 2's desired_set: 1\n",
      "INFO: user 2 choose main itemset.\n",
      "DEBUG: itemset: 1\n",
      "INFO: user: 2, traded items:1\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import networkx as nx \n",
    "import unittest\n",
    "from coupon import Coupon\n",
    "from package.social_graph import SN_Graph\n",
    "from package.model import DiffusionModel\n",
    "import logging\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    testRunner = unittest.TextTestRunner()\n",
    "    suite = unittest.defaultTestLoader.discover(\"./test/\")\n",
    "    testRunner.run(suite)\n",
    "    logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "    \n",
    "    # graph = SN_Graph()\n",
    "    # graph.construct(DIR_PATH + \"edges\", DIR_PATH + \"topic_nodes.csv\")\n",
    "    # subgraph = graph.sampling_subgraph(10)\n",
    "    # nx.write_gml(subgraph, DIR_PATH + \"sample10_graph.gml\")\n",
    "    \n",
    "    \n",
    "    graph = SN_Graph()\n",
    "    graph.add_edge(0, 1, weight=0.01, is_tested=False)\n",
    "    graph.add_edge(0, 2, weight=1, is_tested=False)\n",
    "    for node in graph:\n",
    "        graph.nodes[node]['desired_set'] = None\n",
    "        graph.nodes[node]['adopted_set'] = None\n",
    "            \n",
    "    \n",
    "    topic = {\n",
    "            '0': [0.82, 0.19],\n",
    "            '1': [0.63, 0.37],\n",
    "            '2': [0.5, 0.5]\n",
    "        }\n",
    "    price = [60,260,70]\n",
    "    \n",
    "    model = DiffusionModel(\n",
    "        \"test\",\n",
    "        graph, \n",
    "        {\"price\": price, \"topic\": topic},\n",
    "        [Coupon(180, [0], 20, [0,1]),]\n",
    "    )\n",
    "\n",
    "    \n",
    "    model.diffusion()\n",
    "    model.save(DIR_PATH + \"checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [0.82, 0.19], '1': [0.63, 0.37], '2': [0.5, 0.5]}\n"
     ]
    }
   ],
   "source": [
    "print(model._itemset.TOPIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8398245136169739, 0.16017548638302617]\n",
      "<class 'itemset.Itemset'>\n"
     ]
    }
   ],
   "source": [
    "model.load(DIR_PATH + \"sample10_graph.gml\")\n",
    "print(model._graph.nodes[1][\"topic\"])\n",
    "print(type(model._graph.nodes[2][\"desired_set\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print( model._graph.edges[0,2][\"is_tested\"] == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DASSjciQ1eaa"
   },
   "source": [
    "![image](https://cdn.discordapp.com/attachments/498518865802952706/1019575970103050240/unknown.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1663813895519,
     "user": {
      "displayName": "HeJi Sky",
      "userId": "17996262020646699163"
     },
     "user_tz": -480
    },
    "id": "FgXYsajwo-ST",
    "outputId": "d8ac2059-3c86-4055-fdfe-a785b0cfda0a"
   },
   "outputs": [],
   "source": [
    "with open(DIR_PATH + \"dblp.json\", \"r\") as f:\n",
    "    for line in range(200):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object utf8_encoder at 0x00000204500AF920>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ijson\\backends\\python.py\", line 46, in utf8_encoder\n",
      "    target.close()\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ijson\\backends\\python.py\", line 116, in Lexer\n",
      "    target.send(EOF)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ijson\\backends\\python.py\", line 161, in parse_value\n",
      "    raise common.IncompleteJSONError('Incomplete JSON content')\n",
      "ijson.common.IncompleteJSONError: Incomplete JSON content\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import re\n",
    "import csv\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def extractCorpos(sample=0):\n",
    "    \n",
    "    content = dict()\n",
    "    with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "        parser = ijson.parse(f)\n",
    "        authors = []\n",
    "        '''\n",
    "            將論文的fos跟keywords對應到該篇作者的topic content\n",
    "        '''\n",
    "        count = 0\n",
    "        for prefix, event, value in parser:\n",
    "            if prefix == \"item\" and event == \"start_map\":\n",
    "                authors = []\n",
    "                if sample != 0:\n",
    "                    count += 1\n",
    "                    if count > sample:\n",
    "                        break\n",
    "\n",
    "            if prefix == \"item.authors.item._id\":\n",
    "                authors.append(value)\n",
    "\n",
    "            if prefix ==  \"item.keywords.item\" or prefix == \"item.fos.item\":\n",
    "                text = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', ' ', value).lower()\n",
    "                text = text.replace('\\n', ' ')\n",
    "                for author in authors:\n",
    "                    if author in content and text not in content[author]:\n",
    "                        content[author].append(text)\n",
    "                    else:\n",
    "                        content[author] = [text]\n",
    "\n",
    "    '''\n",
    "        Length of each row is not fixed.\n",
    "\n",
    "        id, string, string, ...\n",
    "    '''\n",
    "    with open(DIR_PATH + \"topic_nodes.csv\", \"w\", encoding=\"utf-8\", newline='') as outputFile:\n",
    "        writer = csv.writer(outputFile)\n",
    "        for author_id, topics in content.items():\n",
    "            topics.insert(0, author_id)\n",
    "            writer.writerow(topics)\n",
    "        \n",
    "extractCorpos()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "\n",
    "with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "    parser = ijson.parse(f)\n",
    "    count = 0\n",
    "    for prefix, event, value in parser:\n",
    "        print(prefix,event,value)\n",
    "        if count > 100:\n",
    "            break\n",
    "        count += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ./nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords', download_dir=\"./nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 1.6899744811276125, 1.7310585786300048], [1.3318122278318338, 0.0, 1.6899744811276125], [1.5744425168116591, 1.549833997312478, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "x_1 = [0.6,0.1,0.3]\n",
    "x_2 = [0.6,0.3,0.1]\n",
    "x_3 = [0.5,0,0.5]\n",
    "\n",
    "R_prime = [[ 0.0, 0.8, 1.0],\n",
    "           [-0.7, 0.0, 0.8],\n",
    "           [ 0.3, 0.2, 0.0],]\n",
    "\n",
    "NUM_ITEMS = 3\n",
    "NUM_TOPICS = 3\n",
    "\n",
    "def transform(matrix):\n",
    "    def sigmoid(num):\n",
    "        return 1+(1/(1+math.exp(-num)))\n",
    "    \n",
    "    for i in range(NUM_ITEMS):\n",
    "        for j in range(NUM_ITEMS):\n",
    "            if i == j:\n",
    "                continue\n",
    "                \n",
    "            matrix[i][j] = sigmoid(matrix[i][j])\n",
    "    return matrix\n",
    "\n",
    "def get_weight(number_item, weights):\n",
    "    total_weights = 0\n",
    "    for i in range(weights):\n",
    "        if i == number_item:\n",
    "            continue\n",
    "        total_weights += weights[i]\n",
    "    return total_weights\n",
    "\n",
    "R_prime = transform(R_prime)\n",
    "print(R_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'53f45728dabfaec09f209538,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''5601754345cedb3395e59457,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''53f38438dabfae4b34a08928,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''5601754345cedb3395e5945a,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''53f43d25dabfaeecd6995149,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''53f46797dabfaeb22f542630,pattern recognition,computer science,feature  computer vision ,document processing,handwriting recognition,optical character recognition,feature extraction,feature  machine learning ,artificial intelligence,intelligent word recognition\\n''54328883dabfaeb4c6a8a699,pattern recognition,computer science,feature  computer vision ,document processing,handwriting recognition,optical character recognition,feature extraction,feature  machine learning ,artificial intelligence,intelligent word recognition\\n''53f43b03dabfaedce555bf2a,statistical static timing analysis,shortest path problem,computer science,algorithm,clock skew,static timing analysis,statistics\\n''53f45ee9dabfaee43ecda842,statistical static timing analysis,shortest path problem,computer science,algorithm,clock skew,static timing analysis,statistics\\n''53f42e8cdabfaee1c0a4274e,statistical static timing analysis,shortest path problem,computer science,algorithm,clock skew,static timing analysis,statistics\\n''53f46946dabfaec09f24b4ed,global high technology,daily short distance flight,enormous waste,daily life\\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''"
     ]
    }
   ],
   "source": [
    "with open(\"./data/dblp/smple10_topic_nodes.csv\", \"r\", encoding=\"utf8\") as f:\n",
    "    sample = []\n",
    "    for i in range(100):\n",
    "        print(repr(f.readline()), end=\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP/WAx3ZLBwLZPjF89nzW9W",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
