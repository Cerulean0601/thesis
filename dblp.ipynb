{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLo9jjs2Iv0H"
   },
   "source": [
    "[Dataset](https://www.aminer.org/citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGX0uzun0lpC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def preprocessing(filename = \"dblpv13.json\"):\n",
    "  '''\n",
    "    將NumberInt(N)置換成整數N\n",
    "\n",
    "    TODO: 111/9/6 bigjson替換成ijson\n",
    "  '''\n",
    "\n",
    "  with open(DIR_PATH + filename, \"r\") as f:\n",
    "    with open(DIR_PATH + \"output\", \"w\") as outputFile:\n",
    "      while True:\n",
    "        content = f.readline()\n",
    "        if content == \"\": \n",
    "          break\n",
    "\n",
    "        erasePosStart = content.find(\"NumberInt(\")\n",
    "        if erasePosStart != -1:\n",
    "          erasePosEnd = content.find(\")\", erasePosStart)\n",
    "          content = content[:erasePosStart] + \\\n",
    "                content[erasePosStart+10 : erasePosEnd] + \\\n",
    "                content[erasePosEnd+1:]\n",
    "\n",
    "        outputFile.write(content)\n",
    "  try:\n",
    "    f = open(DIR_PATH + \"output\", \"rb\")\n",
    "    jsonFormat = bigjson.load(f)\n",
    "    size = len(jsonFormat)\n",
    "  except Exception as e:\n",
    "    print(str(e))\n",
    "    return\n",
    "  else:\n",
    "    os.remove(DIR_PATH + filename)\n",
    "    os.rename(DIR_PATH + \"output\", DIR_PATH + filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njKqns0f5Zqj"
   },
   "outputs": [],
   "source": [
    "import ijson\n",
    "from os.path import exists\n",
    "\n",
    "def captureNodes():\n",
    "    '''\n",
    "    從原始資料集中擷取authers的id作為節點，並且排除掉重複後輸出至node\n",
    "    '''\n",
    "    nodes = set()\n",
    "    with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "        authors = ijson.items(f, \"item\")\n",
    "        author = \"\"\n",
    "        try:\n",
    "            for author in authors:\n",
    "                if \"_id\" in author:\n",
    "                    nodes.add(author[\"_id\"])\n",
    "        except Exception as e:\n",
    "            print(author)\n",
    "          \n",
    "    with open(DIR_PATH + \"nodes\", \"w\") as output:\n",
    "        for node in nodes:\n",
    "            output.write(\"\\\"\" + node + \"\\\"\\n\")\n",
    "\n",
    "def captureEdges():\n",
    "    '''\n",
    "    若任兩個節點只要有一篇共同著作，則視為有邊連在一起\n",
    "    '''\n",
    "    output = open(DIR_PATH + \"edges\", \"w\")\n",
    "    with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "        parser = ijson.parse(f)\n",
    "        for prefix, event, value in parser:\n",
    "            if prefix == \"item.authors\" and event == \"start_array\":\n",
    "                authorsInSamePaper = set()\n",
    "                while event != \"end_array\":\n",
    "                    prefix, event, value = next(parser)\n",
    "                    if prefix == \"item.authors.item._id\":\n",
    "                        authorsInSamePaper.add(value)\n",
    "                        \n",
    "\n",
    "        for c in combinations(authorsInSamePaper, 2):\n",
    "            output.write(c[0] + \",\" + c[1] + \"\\n\")\n",
    "\n",
    "def extractTopic():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15561,
     "status": "ok",
     "timestamp": 1665457440579,
     "user": {
      "displayName": "HeJi Sky",
      "userId": "17996262020646699163"
     },
     "user_tz": -480
    },
    "id": "8pZsK6w6Z5aM",
    "outputId": "c8fd8f63-1fb8-4c0a-860a-316ba3f9c56a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DIR_PATH = \"\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/dirve\")\n",
    "    DIR_PATH = r\"/content/dirve/MyDrive/研究所/Data/dblp/\"\n",
    "    sys.path.append('/content/dirve/MyDrive/Colab Notebooks/package')\n",
    "else:\n",
    "    DIR_PATH = r\"D:\\\\論文實驗\\\\data\\\\dblp\\\\\"\n",
    "    sys.path.append('D:\\\\論文實驗\\\\package')\n",
    "    sys.path.append(\"D:\\\\論文實驗\\\\env\\\\Lib\\\\site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1664524239200,
     "user": {
      "displayName": "HeJi Sky",
      "userId": "17996262020646699163"
     },
     "user_tz": -480
    },
    "id": "PY3-d9YdvJyV",
    "outputId": "35271272-7498-4779-b013-bf02c81f069e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.009s\n",
      "\n",
      "OK\n",
      "DEBUG: Allocate 1 to 0\n",
      "INFO: Allocation is complete.\n",
      "INFO: user 0 choose main itemset 1.\n",
      "INFO: user: 0, traded items:1\n",
      "INFO: 0 tries to activate 1: False\n",
      "INFO: 0 tries to activate 2: True\n",
      "DEBUG: 2's desired_set: 1\n",
      "INFO: user 2 choose main itemset 1.\n",
      "INFO: user: 2, traded items:1\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import networkx as nx \n",
    "import unittest\n",
    "from coupon import Coupon\n",
    "from social_graph import SN_Graph\n",
    "from model import DiffusionModel\n",
    "import logging\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    testRunner = unittest.TextTestRunner()\n",
    "    suite = unittest.defaultTestLoader.discover(\"./test/\")\n",
    "    testRunner.run(suite)\n",
    "    logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "    \n",
    "    # graph = SN_Graph()\n",
    "    # graph.construct(DIR_PATH + \"edges\", DIR_PATH + \"topic_nodes.csv\")\n",
    "    # subgraph = graph.sampling_subgraph(10)\n",
    "    # nx.write_gml(subgraph, DIR_PATH + \"sample10_graph.gml\")\n",
    "    \n",
    "    \n",
    "    graph = SN_Graph()\n",
    "    graph.add_edge(0, 1, weight=0.01, is_tested=False)\n",
    "    graph.add_edge(0, 2, weight=1, is_tested=False)\n",
    "    for node in graph:\n",
    "        graph.nodes[node]['desired_set'] = None\n",
    "        graph.nodes[node]['adopted_set'] = None\n",
    "            \n",
    "    \n",
    "    topic = {\n",
    "            '0': [0.82, 0.19],\n",
    "            '1': [0.63, 0.37],\n",
    "            '2': [0.5, 0.5]\n",
    "        }\n",
    "    price = [60,260,70]\n",
    "    \n",
    "    model = DiffusionModel(\n",
    "        \"test\",\n",
    "        graph, \n",
    "        {\"price\": price, \"topic\": topic},\n",
    "        [Coupon(180, [0], 20, [0,1]),]\n",
    "    )\n",
    "\n",
    "    \n",
    "    model.diffusion()\n",
    "    model.save(DIR_PATH + \"checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [0.82, 0.19], '1': [0.63, 0.37], '2': [0.5, 0.5]}\n"
     ]
    }
   ],
   "source": [
    "print(model._itemset.TOPIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8398245136169739, 0.16017548638302617]\n",
      "<class 'itemset.Itemset'>\n"
     ]
    }
   ],
   "source": [
    "model.load(DIR_PATH + \"sample10_graph.gml\")\n",
    "print(model._graph.nodes[1][\"topic\"])\n",
    "print(type(model._graph.nodes[2][\"desired_set\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print( model._graph.edges[0,2][\"is_tested\"] == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DASSjciQ1eaa"
   },
   "source": [
    "![image](https://cdn.discordapp.com/attachments/498518865802952706/1019575970103050240/unknown.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1663813895519,
     "user": {
      "displayName": "HeJi Sky",
      "userId": "17996262020646699163"
     },
     "user_tz": -480
    },
    "id": "FgXYsajwo-ST",
    "outputId": "d8ac2059-3c86-4055-fdfe-a785b0cfda0a"
   },
   "outputs": [],
   "source": [
    "with open(DIR_PATH + \"dblp.json\", \"r\") as f:\n",
    "    for line in range(200):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract topics from DBLP dataset\n",
    "### Note\n",
    "- **未清除keywords和fos都沒有值的Paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "import csv\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def extractUsersCorpos(sample=0):\n",
    "    \n",
    "    content = dict()\n",
    "    with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "        parser = ijson.parse(f)\n",
    "        authors = []\n",
    "        '''\n",
    "            將論文的fos跟keywords對應到該篇作者的topic content\n",
    "        '''\n",
    "        count = 0\n",
    "        for prefix, event, value in parser:\n",
    "            if prefix == \"item\" and event == \"start_map\":\n",
    "                authors = []\n",
    "                if sample != 0:\n",
    "                    count += 1\n",
    "                    if count > sample:\n",
    "                        break\n",
    "\n",
    "            if prefix == \"item.authors.item._id\":\n",
    "                authors.append(value)\n",
    "\n",
    "            if prefix ==  \"item.keywords.item\" or prefix == \"item.fos.item\":\n",
    "                text = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', ' ', value).lower()\n",
    "                text = text.replace('\\n', ' ')\n",
    "                for author in authors:\n",
    "                    if author in content and text not in content[author]:\n",
    "                        content[author].append(text)\n",
    "                    else:\n",
    "                        content[author] = [text]\n",
    "\n",
    "    '''\n",
    "        Length of each row is not fixed.\n",
    "\n",
    "        id, string, string, ...\n",
    "    '''\n",
    "    filename = DIR_PATH + \"topic_nodes.csv\" if sample == 0 else DIR_PATH + \"sample\" + str(sample) + \"topic_nodes.csv\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline='') as outputFile:\n",
    "        writer = csv.writer(outputFile)\n",
    "        for author_id, topics in content.items():\n",
    "            topics.insert(0, author_id)\n",
    "            writer.writerow(topics)\n",
    "        \n",
    "def extractItemsCorpos(sample=0):\n",
    "    content = dict()\n",
    "    with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "        parser = ijson.parse(f)\n",
    "        '''\n",
    "            將論文的fos跟keywords對應到該篇的_id\n",
    "        '''\n",
    "        count = 0\n",
    "        for prefix, event, value in parser:\n",
    "            \n",
    "            if sample != 0 and count > sample - 1:\n",
    "                break\n",
    "                \n",
    "            if prefix == \"item\" and event == \"end_map\" and paper_id in content:\n",
    "                count += 1\n",
    "                \n",
    "            if prefix == \"item._id\":\n",
    "                paper_id = value\n",
    "                print(value)\n",
    "                \n",
    "            if prefix ==  \"item.keywords.item\" or prefix == \"item.fos.item\":\n",
    "                text = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', ' ', value).lower()\n",
    "                text = text.replace('\\n', ' ')\n",
    "                if paper_id in content and text not in content[paper_id]:\n",
    "                    content[paper_id].append(text)\n",
    "                else:\n",
    "                    content[paper_id] = [text]\n",
    "                    \n",
    "            \n",
    "                \n",
    "\n",
    "    '''\n",
    "        Length of each row is not fixed.\n",
    "\n",
    "        id, string, string, ...\n",
    "    '''\n",
    "    filename = DIR_PATH + \"topic_items.csv\" if sample == 0 else DIR_PATH + \"sample\" + str(sample) + \"topic_items.csv\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline='') as outputFile:\n",
    "        writer = csv.writer(outputFile)\n",
    "        for paper_id, topics in content.items():\n",
    "            topics.insert(0, paper_id)\n",
    "            writer.writerow(topics)\n",
    "\n",
    "extractItemsCorpos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834826\n"
     ]
    }
   ],
   "source": [
    "with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "    parser = ijson.parse(f)\n",
    "    count = 0\n",
    "    for prefix, event, value in parser:        \n",
    "        if prefix == \"item\" and event == \"start_map\":\n",
    "            count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " start_array None\n",
      "item start_map None\n",
      "item map_key _id\n",
      "item._id string 53e99784b7602d9701f3e3f5\n",
      "item map_key title\n",
      "item.title string 3GIO.\n",
      "item map_key venue\n",
      "item.venue start_map None\n",
      "item.venue map_key type\n",
      "item.venue.type number 0\n",
      "item.venue end_map None\n",
      "item map_key year\n",
      "item.year number 2011\n",
      "item map_key keywords\n",
      "item.keywords start_array None\n",
      "item.keywords end_array None\n",
      "item map_key n_citation\n",
      "item.n_citation number 0\n",
      "item map_key lang\n",
      "item.lang string en\n",
      "item end_map None\n",
      "item start_map None\n",
      "item map_key _id\n",
      "item._id string 53e99784b7602d9701f3e133\n",
      "item map_key title\n",
      "item.title string The relationship between canopy parameters and spectrum of winter wheat under different irrigations in Hebei Province.\n",
      "item map_key authors\n",
      "item.authors start_array None\n",
      "item.authors.item start_map None\n",
      "item.authors.item map_key _id\n",
      "item.authors.item._id string 53f45728dabfaec09f209538\n",
      "item.authors.item map_key name\n",
      "item.authors.item.name string Peijuan Wang\n",
      "item.authors.item end_map None\n",
      "item.authors.item start_map None\n",
      "item.authors.item map_key _id\n",
      "item.authors.item._id string 5601754345cedb3395e59457\n",
      "item.authors.item map_key name\n",
      "item.authors.item.name string Jiahua Zhang\n",
      "item.authors.item end_map None\n",
      "item.authors.item start_map None\n",
      "item.authors.item map_key _id\n",
      "item.authors.item._id string 53f38438dabfae4b34a08928\n",
      "item.authors.item map_key name\n",
      "item.authors.item.name string Donghui Xie\n",
      "item.authors.item end_map None\n",
      "item.authors.item start_map None\n",
      "item.authors.item map_key _id\n",
      "item.authors.item._id string 5601754345cedb3395e5945a\n",
      "item.authors.item map_key name\n",
      "item.authors.item.name string Yanyan Xu\n",
      "item.authors.item end_map None\n",
      "item.authors.item start_map None\n",
      "item.authors.item map_key _id\n",
      "item.authors.item._id string 53f43d25dabfaeecd6995149\n",
      "item.authors.item map_key name\n",
      "item.authors.item.name string Yun Xu\n",
      "item.authors.item end_map None\n",
      "item.authors end_array None\n",
      "item map_key venue\n",
      "item.venue start_map None\n",
      "item.venue map_key _id\n",
      "item.venue._id string 53a7297d20f7420be8bd4ae7\n",
      "item.venue map_key name_d\n",
      "item.venue.name_d string International Geoscience and Remote Sensing Symposium\n",
      "item.venue map_key type\n",
      "item.venue.type number 0\n",
      "item.venue map_key raw\n",
      "item.venue.raw string IGARSS\n",
      "item.venue end_map None\n",
      "item map_key year\n",
      "item.year number 2011\n",
      "item map_key keywords\n",
      "item.keywords start_array None\n",
      "item.keywords.item string canopy parameters\n",
      "item.keywords.item string canopy spectrum\n",
      "item.keywords.item string different soil water content control\n",
      "item.keywords.item string winter wheat\n",
      "item.keywords.item string irrigation\n",
      "item.keywords.item string hydrology\n",
      "item.keywords.item string radiometry\n",
      "item.keywords.item string moisture\n",
      "item.keywords.item string indexes\n",
      "item.keywords.item string vegetation\n",
      "item.keywords.item string indexation\n",
      "item.keywords.item string dry weight\n",
      "item.keywords.item string soil moisture\n",
      "item.keywords.item string water content\n",
      "item.keywords.item string indexing terms\n",
      "item.keywords.item string spectrum\n",
      "item.keywords.item string natural disaster\n",
      "item.keywords end_array None\n",
      "item map_key fos\n",
      "item.fos start_array None\n",
      "item.fos.item string Agronomy\n",
      "item.fos.item string Moisture\n",
      "item.fos.item string Hydrology\n",
      "item.fos.item string Environmental science\n",
      "item.fos.item string Dry weight\n",
      "item.fos.item string Water content\n",
      "item.fos.item string Stomatal conductance\n",
      "item.fos.item string Transpiration\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import re\n",
    "\n",
    "with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "    parser = ijson.parse(f)\n",
    "    count = 0\n",
    "    for prefix, event, value in parser:\n",
    "        print(prefix,event,value)\n",
    "        if count > 100:\n",
    "            break\n",
    "        count += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ./nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords', download_dir=\"./nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 1.6899744811276125, 1.7310585786300048], [1.3318122278318338, 0.0, 1.6899744811276125], [1.5744425168116591, 1.549833997312478, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "x_1 = [0.6,0.1,0.3]\n",
    "x_2 = [0.6,0.3,0.1]\n",
    "x_3 = [0.5,0,0.5]\n",
    "\n",
    "R_prime = [[ 0.0, 0.8, 1.0],\n",
    "           [-0.7, 0.0, 0.8],\n",
    "           [ 0.3, 0.2, 0.0],]\n",
    "\n",
    "NUM_ITEMS = 3\n",
    "NUM_TOPICS = 3\n",
    "\n",
    "def transform(matrix):\n",
    "    def sigmoid(num):\n",
    "        return 1+(1/(1+math.exp(-num)))\n",
    "    \n",
    "    for i in range(NUM_ITEMS):\n",
    "        for j in range(NUM_ITEMS):\n",
    "            if i == j:\n",
    "                continue\n",
    "                \n",
    "            matrix[i][j] = sigmoid(matrix[i][j])\n",
    "    return matrix\n",
    "\n",
    "def get_weight(number_item, weights):\n",
    "    total_weights = 0\n",
    "    for i in range(weights):\n",
    "        if i == number_item:\n",
    "            continue\n",
    "        total_weights += weights[i]\n",
    "    return total_weights\n",
    "\n",
    "R_prime = transform(R_prime)\n",
    "print(R_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'53f45728dabfaec09f209538,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''5601754345cedb3395e59457,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''53f38438dabfae4b34a08928,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''5601754345cedb3395e5945a,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''53f43d25dabfaeecd6995149,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''53f46797dabfaeb22f542630,pattern recognition,computer science,feature  computer vision ,document processing,handwriting recognition,optical character recognition,feature extraction,feature  machine learning ,artificial intelligence,intelligent word recognition\\n''54328883dabfaeb4c6a8a699,pattern recognition,computer science,feature  computer vision ,document processing,handwriting recognition,optical character recognition,feature extraction,feature  machine learning ,artificial intelligence,intelligent word recognition\\n''53f43b03dabfaedce555bf2a,statistical static timing analysis,shortest path problem,computer science,algorithm,clock skew,static timing analysis,statistics\\n''53f45ee9dabfaee43ecda842,statistical static timing analysis,shortest path problem,computer science,algorithm,clock skew,static timing analysis,statistics\\n''53f42e8cdabfaee1c0a4274e,statistical static timing analysis,shortest path problem,computer science,algorithm,clock skew,static timing analysis,statistics\\n''53f46946dabfaec09f24b4ed,global high technology,daily short distance flight,enormous waste,daily life\\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''"
     ]
    }
   ],
   "source": [
    "with open(\"./data/dblp/smple10_topic_nodes.csv\", \"r\", encoding=\"utf8\") as f:\n",
    "    sample = []\n",
    "    for i in range(100):\n",
    "        print(repr(f.readline()), end=\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP/WAx3ZLBwLZPjF89nzW9W",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "02780483269052b0ddef6a1fd82b82d2be0dd99a863ba9a6369e5e299a35d3c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
