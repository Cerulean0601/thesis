{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLo9jjs2Iv0H"
   },
   "source": [
    "[Dataset](https://www.aminer.org/citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGX0uzun0lpC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def preprocessing(filename = \"dblpv13.json\"):\n",
    "  '''\n",
    "    將NumberInt(N)置換成整數N\n",
    "\n",
    "    TODO: 111/9/6 bigjson替換成ijson\n",
    "  '''\n",
    "\n",
    "  with open(DIR_PATH + filename, \"r\") as f:\n",
    "    with open(DIR_PATH + \"output\", \"w\") as outputFile:\n",
    "      while True:\n",
    "        content = f.readline()\n",
    "        if content == \"\": \n",
    "          break\n",
    "\n",
    "        erasePosStart = content.find(\"NumberInt(\")\n",
    "        if erasePosStart != -1:\n",
    "          erasePosEnd = content.find(\")\", erasePosStart)\n",
    "          content = content[:erasePosStart] + \\\n",
    "                content[erasePosStart+10 : erasePosEnd] + \\\n",
    "                content[erasePosEnd+1:]\n",
    "\n",
    "        outputFile.write(content)\n",
    "  try:\n",
    "    f = open(DIR_PATH + \"output\", \"rb\")\n",
    "    jsonFormat = bigjson.load(f)\n",
    "    size = len(jsonFormat)\n",
    "  except Exception as e:\n",
    "    print(str(e))\n",
    "    return\n",
    "  else:\n",
    "    os.remove(DIR_PATH + filename)\n",
    "    os.rename(DIR_PATH + \"output\", DIR_PATH + filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njKqns0f5Zqj"
   },
   "outputs": [],
   "source": [
    "import ijson\n",
    "from os.path import exists\n",
    "\n",
    "def captureNodes():\n",
    "    '''\n",
    "    從原始資料集中擷取authers的id作為節點，並且排除掉重複後輸出至node\n",
    "    '''\n",
    "    nodes = set()\n",
    "    with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "        authors = ijson.items(f, \"item\")\n",
    "        author = \"\"\n",
    "        try:\n",
    "            for author in authors:\n",
    "                if \"_id\" in author:\n",
    "                    nodes.add(author[\"_id\"])\n",
    "        except Exception as e:\n",
    "            print(author)\n",
    "          \n",
    "    with open(DIR_PATH + \"nodes\", \"w\") as output:\n",
    "        for node in nodes:\n",
    "            output.write(\"\\\"\" + node + \"\\\"\\n\")\n",
    "\n",
    "def captureEdges():\n",
    "    '''\n",
    "    若任兩個節點只要有一篇共同著作，則視為有邊連在一起\n",
    "    '''\n",
    "    output = open(DIR_PATH + \"edges\", \"w\")\n",
    "    with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "        parser = ijson.parse(f)\n",
    "        for prefix, event, value in parser:\n",
    "            if prefix == \"item.authors\" and event == \"start_array\":\n",
    "                authorsInSamePaper = set()\n",
    "                while event != \"end_array\":\n",
    "                    prefix, event, value = next(parser)\n",
    "                    if prefix == \"item.authors.item._id\":\n",
    "                        authorsInSamePaper.add(value)\n",
    "                        \n",
    "\n",
    "        for c in combinations(authorsInSamePaper, 2):\n",
    "            output.write(c[0] + \",\" + c[1] + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15561,
     "status": "ok",
     "timestamp": 1665457440579,
     "user": {
      "displayName": "HeJi Sky",
      "userId": "17996262020646699163"
     },
     "user_tz": -480
    },
    "id": "8pZsK6w6Z5aM",
    "outputId": "c8fd8f63-1fb8-4c0a-860a-316ba3f9c56a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DIR_PATH = \"\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/dirve\")\n",
    "    DIR_PATH = r\"/content/dirve/MyDrive/研究所/Data/dblp/\"\n",
    "    sys.path.append('/content/dirve/MyDrive/Colab Notebooks/package')\n",
    "else:\n",
    "    DIR_PATH = r\"D:\\\\論文實驗\\\\data\\\\dblp\\\\\"\n",
    "    sys.path.append('D:\\\\論文實驗\\\\package')\n",
    "    sys.path.append(\"D:\\\\論文實驗\\\\env\\\\Lib\\\\site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1664524239200,
     "user": {
      "displayName": "HeJi Sky",
      "userId": "17996262020646699163"
     },
     "user_tz": -480
    },
    "id": "PY3-d9YdvJyV",
    "outputId": "35271272-7498-4779-b013-bf02c81f069e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'coupon'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcoupon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Coupon\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msocial_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SN_Graph\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiffusionModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'coupon'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import networkx as nx \n",
    "import unittest\n",
    "from coupon import Coupon\n",
    "from social_graph import SN_Graph\n",
    "from model import DiffusionModel\n",
    "import logging\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    testRunner = unittest.TextTestRunner()\n",
    "    suite = unittest.defaultTestLoader.discover(\"./test/\")\n",
    "    testRunner.run(suite)\n",
    "    logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "    \n",
    "    # graph = SN_Graph()\n",
    "    # graph.construct(DIR_PATH + \"edges\", DIR_PATH + \"topic_nodes.csv\")\n",
    "    # subgraph = graph.sampling_subgraph(10)\n",
    "    # nx.write_gml(subgraph, DIR_PATH + \"sample10_graph.gml\")\n",
    "    \n",
    "    \n",
    "    graph = SN_Graph()\n",
    "    graph.add_edge(0, 1, weight=0.01, is_tested=False)\n",
    "    graph.add_edge(0, 2, weight=1, is_tested=False)\n",
    "    for node in graph:\n",
    "        graph.nodes[node]['desired_set'] = None\n",
    "        graph.nodes[node]['adopted_set'] = None\n",
    "            \n",
    "    \n",
    "    topic = {\n",
    "            '0': [0.82, 0.19],\n",
    "            '1': [0.63, 0.37],\n",
    "            '2': [0.5, 0.5]\n",
    "        }\n",
    "    price = [60,260,70]\n",
    "    \n",
    "    model = DiffusionModel(\n",
    "        \"test\",\n",
    "        graph, \n",
    "        {\"price\": price, \"topic\": topic},\n",
    "        [Coupon(180, [0], 20, [0,1]),]\n",
    "    )\n",
    "\n",
    "    \n",
    "    model.diffusion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [0.82, 0.19], '1': [0.63, 0.37], '2': [0.5, 0.5]}\n"
     ]
    }
   ],
   "source": [
    "print(model._itemset.TOPIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8398245136169739, 0.16017548638302617]\n",
      "<class 'itemset.Itemset'>\n"
     ]
    }
   ],
   "source": [
    "model.load(DIR_PATH + \"sample10_graph.gml\")\n",
    "print(model._graph.nodes[1][\"topic\"])\n",
    "print(type(model._graph.nodes[2][\"desired_set\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print( model._graph.edges[0,2][\"is_tested\"] == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DASSjciQ1eaa"
   },
   "source": [
    "![image](https://cdn.discordapp.com/attachments/498518865802952706/1019575970103050240/unknown.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1663813895519,
     "user": {
      "displayName": "HeJi Sky",
      "userId": "17996262020646699163"
     },
     "user_tz": -480
    },
    "id": "FgXYsajwo-ST",
    "outputId": "d8ac2059-3c86-4055-fdfe-a785b0cfda0a"
   },
   "outputs": [],
   "source": [
    "with open(DIR_PATH + \"dblp.json\", \"r\") as f:\n",
    "    for line in range(200):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract topics from DBLP dataset\n",
    "### Note\n",
    "- ~~未清除keywords和fos都沒有值的Paper~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "import csv\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def extractUsersCorpos(sample=0):\n",
    "    \n",
    "    content = dict()\n",
    "    with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "        parser = ijson.parse(f)\n",
    "        authors = []\n",
    "        '''\n",
    "            將論文的fos跟keywords對應到該篇作者的topic content\n",
    "        '''\n",
    "        count = 0\n",
    "        for prefix, event, value in parser:\n",
    "            if prefix == \"item\" and event == \"start_map\":\n",
    "                authors = []\n",
    "                if sample != 0:\n",
    "                    count += 1\n",
    "                    if count > sample:\n",
    "                        break\n",
    "\n",
    "            if prefix == \"item.authors.item._id\":\n",
    "                authors.append(value)\n",
    "\n",
    "            if prefix ==  \"item.keywords.item\" or prefix == \"item.fos.item\":\n",
    "                text = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', ' ', value).lower()\n",
    "                text = text.replace('\\n', ' ')\n",
    "                for author in authors:\n",
    "                    if author in content and text not in content[author]:\n",
    "                        content[author].append(text)\n",
    "                    else:\n",
    "                        content[author] = [text]\n",
    "\n",
    "    '''\n",
    "        Length of each row is not fixed.\n",
    "\n",
    "        id, string, string, ...\n",
    "    '''\n",
    "    filename = DIR_PATH + \"topic_nodes.csv\" if sample == 0 else DIR_PATH + \"sample\" + str(sample) + \"topic_nodes.csv\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline='') as outputFile:\n",
    "        writer = csv.writer(outputFile)\n",
    "        for author_id, topics in content.items():\n",
    "            topics.insert(0, author_id)\n",
    "            writer.writerow(topics)\n",
    "        \n",
    "# def extractItemsCorpos(sample=0):\n",
    "#     content = dict()\n",
    "#     with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "#         parser = ijson.parse(f)\n",
    "#         '''\n",
    "#             將論文的fos跟keywords對應到該篇的_id\n",
    "#         '''\n",
    "#         count = 0\n",
    "#         for prefix, event, value in parser:\n",
    "            \n",
    "#             if sample != 0 and count > sample - 1:\n",
    "#                 break\n",
    "                \n",
    "#             if prefix == \"item\" and event == \"end_map\" and paper_id in content:\n",
    "#                 count += 1\n",
    "                \n",
    "#             if prefix == \"item._id\":\n",
    "#                 paper_id = value\n",
    "#                 print(value)\n",
    "                \n",
    "#             if prefix ==  \"item.keywords.item\" or prefix == \"item.fos.item\":\n",
    "#                 text = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', ' ', value).lower()\n",
    "#                 text = text.replace('\\n', ' ')\n",
    "#                 if paper_id in content and text not in content[paper_id]:\n",
    "#                     content[paper_id].append(text)\n",
    "#                 else:\n",
    "#                     content[paper_id] = [text]\n",
    "                    \n",
    "            \n",
    "                \n",
    "\n",
    "#     '''\n",
    "#         Length of each row is not fixed.\n",
    "\n",
    "#         id, string, string, ...\n",
    "#     '''\n",
    "#     filename = DIR_PATH + \"topic_items.csv\" if sample == 0 else DIR_PATH + \"sample\" + str(sample) + \"topic_items.csv\"\n",
    "#     with open(filename, \"w\", encoding=\"utf-8\", newline='') as outputFile:\n",
    "#         writer = csv.writer(outputFile)\n",
    "#         for paper_id, topics in content.items():\n",
    "#             topics.insert(0, paper_id)\n",
    "#             writer.writerow(topics)\n",
    "\n",
    "extractUsersCorpos(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834826\n"
     ]
    }
   ],
   "source": [
    "with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "    parser = ijson.parse(f)\n",
    "    count = 0\n",
    "    for prefix, event, value in parser:        \n",
    "        if prefix == \"item\" and event == \"start_map\":\n",
    "            count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "\n",
    "with open(DIR_PATH + \"dblp.json\", \"rb\") as f:\n",
    "    parser = ijson.parse(f)\n",
    "    count = 0\n",
    "    for prefix, event, value in parser:\n",
    "        print(prefix,event,value)\n",
    "        if count > 100:\n",
    "            break\n",
    "        count += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ./nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords', download_dir=\"./nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 1.6899744811276125, 1.7310585786300048], [1.3318122278318338, 0.0, 1.6899744811276125], [1.5744425168116591, 1.549833997312478, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "x_1 = [0.6,0.1,0.3]\n",
    "x_2 = [0.6,0.3,0.1]\n",
    "x_3 = [0.5,0,0.5]\n",
    "\n",
    "R_prime = [[ 0.0, 0.8, 1.0],\n",
    "           [-0.7, 0.0, 0.8],\n",
    "           [ 0.3, 0.2, 0.0],]\n",
    "\n",
    "NUM_ITEMS = 3\n",
    "NUM_TOPICS = 3\n",
    "\n",
    "def transform(matrix):\n",
    "    def sigmoid(num):\n",
    "        return 1+(1/(1+math.exp(-num)))\n",
    "    \n",
    "    for i in range(NUM_ITEMS):\n",
    "        for j in range(NUM_ITEMS):\n",
    "            if i == j:\n",
    "                continue\n",
    "                \n",
    "            matrix[i][j] = sigmoid(matrix[i][j])\n",
    "    return matrix\n",
    "\n",
    "def get_weight(number_item, weights):\n",
    "    total_weights = 0\n",
    "    for i in range(weights):\n",
    "        if i == number_item:\n",
    "            continue\n",
    "        total_weights += weights[i]\n",
    "    return total_weights\n",
    "\n",
    "R_prime = transform(R_prime)\n",
    "print(R_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'53f45728dabfaec09f209538,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''5601754345cedb3395e59457,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''53f38438dabfae4b34a08928,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''5601754345cedb3395e5945a,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''53f43d25dabfaeecd6995149,moisture,hydrology,environmental science,dry weight,water content,stomatal conductance,transpiration,irrigation,soil water,canopy\\n''53f46797dabfaeb22f542630,pattern recognition,computer science,feature  computer vision ,document processing,handwriting recognition,optical character recognition,feature extraction,feature  machine learning ,artificial intelligence,intelligent word recognition\\n''54328883dabfaeb4c6a8a699,pattern recognition,computer science,feature  computer vision ,document processing,handwriting recognition,optical character recognition,feature extraction,feature  machine learning ,artificial intelligence,intelligent word recognition\\n''53f43b03dabfaedce555bf2a,statistical static timing analysis,shortest path problem,computer science,algorithm,clock skew,static timing analysis,statistics\\n''53f45ee9dabfaee43ecda842,statistical static timing analysis,shortest path problem,computer science,algorithm,clock skew,static timing analysis,statistics\\n''53f42e8cdabfaee1c0a4274e,statistical static timing analysis,shortest path problem,computer science,algorithm,clock skew,static timing analysis,statistics\\n''53f46946dabfaec09f24b4ed,global high technology,daily short distance flight,enormous waste,daily life\\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''"
     ]
    }
   ],
   "source": [
    "with open(\"./data/dblp/smple10_topic_nodes.csv\", \"r\", encoding=\"utf8\") as f:\n",
    "    sample = []\n",
    "    for i in range(100):\n",
    "        print(repr(f.readline()), end=\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP/WAx3ZLBwLZPjF89nzW9W",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "02780483269052b0ddef6a1fd82b82d2be0dd99a863ba9a6369e5e299a35d3c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
