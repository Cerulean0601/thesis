{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2a4755",
   "metadata": {},
   "source": [
    "The [dataset](https://jmcauley.ucsd.edu/data/amazon_v2/index.html) is metadata for Magazine Subscriptions category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3922f187-f704-4399-97eb-3db8aaad7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DIR_PATH = \"\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/dirve\")\n",
    "    DIR_PATH = r\"/content/dirve/MyDrive/研究所/Data/dblp/\"\n",
    "    sys.path.append('/content/dirve/MyDrive/Colab Notebooks/package')\n",
    "else:\n",
    "    DIR_PATH = r\"D:\\\\論文實驗\\\\data\\\\dblp\\\\\"\n",
    "    sys.path.append('D:\\\\論文實驗\\\\package')\n",
    "    sys.path.append(\"D:\\\\論文實驗\\\\env\\\\Lib\\\\site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8854303-1b98-414c-a583-a70752f4a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import preprocessingText\n",
    "\n",
    "DIR_PATH = r\"D:\\\\論文實驗\\\\data\\\\amazon\\\\\"\n",
    "DATASET = r\"meta_Software.json\"\n",
    "\n",
    "def checkAttr():\n",
    "    '''\n",
    "        check whether ASIN of the item is existing.\n",
    "        \n",
    "        Result: \n",
    "            all of the items exist ASIN of the properties.\n",
    "    '''\n",
    "    with open(DIR_PATH + DATASET, \"r\") as f:\n",
    "        data = pd.read_json(f, orient=\"records\", typ=\"series\", lines=True)\n",
    "    \n",
    "        for item in data:\n",
    "            if not all(key in item for key in [\"asin\", \"also_view\", \"also_buy\", \"category\", \"price\"]):\n",
    "                print(item)\n",
    "\n",
    "def reduceAttr(output_file):\n",
    "    '''\n",
    "        保留需要的商品屬性, asin, also_view, also_buy, category, price.\n",
    "        also_view和also_buy有多個以上時, 用空白隔開\n",
    "    '''\n",
    "    \n",
    "    with open(DIR_PATH + DATASET, \"r\") as f:\n",
    "        data = pd.read_json(f, orient=\"records\", typ=\"series\", lines=True, dtype={\"category\":list})\n",
    "\n",
    "        output = open(DIR_PATH + output_file, \"w\")\n",
    "        for item in data:\n",
    "            also_view = \" \".join(item[\"also_view\"])\n",
    "            also_buy = \" \".join(item[\"also_buy\"])\n",
    "            category = \" \".join([preprocessingText(string) for string in item[\"category\"]])\n",
    "            price = item[\"price\"][1:] if item[\"price\"] != \"\" and item[\"price\"][0] == \"$\" else \"\"\n",
    "\n",
    "            if also_view and also_buy and category and price:\n",
    "                row = \"{asin},{also_view},{also_buy},{category},{price}\".format(\n",
    "                    asin = item[\"asin\"], \n",
    "                    also_view = also_view, \n",
    "                    also_buy = also_buy, \n",
    "                    category = category, \n",
    "                    price = price.replace(\",\", \"\"))\n",
    "                output.write(row + \"\\n\")\n",
    "        output.close()\n",
    "checkAttr()\n",
    "reduceAttr(\"extraction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f99070-2fa6-44f6-9899-410c600a0ddb",
   "metadata": {},
   "source": [
    "### 需要保留的商品屬性\n",
    "- asin\n",
    "- also_view\n",
    "- also_buy\n",
    "- category\n",
    "- price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793ca8f-f09d-4b68-a210-5c87a9ad58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DIR_PATH + DATASET, \"r\") as f:\n",
    "    data = pd.read_json(f, orient=\"records\", typ=\"series\", lines=True)\n",
    "    count = 0\n",
    "    for item in data:\n",
    "        if item[\"asin\"] == \"0078605407\":\n",
    "            count += 1\n",
    "            also_view = \" \".join(item[\"also_view\"])\n",
    "            also_buy = \" \".join(item[\"also_buy\"])\n",
    "            category = \" \".join([preprocessingText(string) for string in item[\"category\"]])\n",
    "            price = item[\"price\"][1:] if item[\"price\"] != \"\" and item[\"price\"][0] == \"$\" else \"\"\n",
    "            \n",
    "            print(\"{asin},{also_view},{also_buy},{category},{price}\".format(\n",
    "                    asin = item[\"asin\"], \n",
    "                    also_view = also_view, \n",
    "                    also_buy = also_buy, \n",
    "                    category = category, \n",
    "                    price = str(price)))\n",
    "#print(\"{0}/{1}\".format(count, data.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be27f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DIR_PATH + \"extraction.csv\", \"r\") as f:\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        count = count + 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c671ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_items(filename):\n",
    "    dataset = dict()\n",
    "    with open(filename) as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            asin, also_view, also_buy, category, price = line.split(\",\")\n",
    "            dataset[asin] = dict()\n",
    "            dataset[asin][\"also_view\"] = also_view.split(\" \")\n",
    "            dataset[asin][\"also_buy\"] = also_buy.split(\" \")\n",
    "            dataset[asin][\"category\"] = category.split(\" \")\n",
    "            dataset[asin][\"price\"] = float(price)\n",
    "            dataset[asin][\"freq\"] = 1\n",
    "\n",
    "    for asin, attr in dataset.items():\n",
    "        filter_list = []\n",
    "        for substitution in attr[\"also_view\"]:\n",
    "            if substitution in dataset:\n",
    "                filter_list.append(substitution)\n",
    "                dataset[substitution][\"freq\"] += 1\n",
    "\n",
    "        dataset[asin][\"also_view\"] = filter_list\n",
    "\n",
    "        filter_list = []\n",
    "        for complementary in attr[\"also_buy\"]:\n",
    "            if complementary in dataset:\n",
    "                filter_list.append(complementary)\n",
    "                dataset[complementary][\"freq\"] += 1\n",
    "\n",
    "        dataset[asin][\"also_buy\"] = filter_list\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def maxFrequency(filename):\n",
    "    dataset = dict()\n",
    "\n",
    "    with open(filename) as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            asin, also_view, also_buy, category, price = line.split(\",\")\n",
    "            dataset[asin] = dict()\n",
    "            dataset[asin][\"also_view\"] = also_view.split(\" \")\n",
    "            dataset[asin][\"also_buy\"] = also_buy.split(\" \")\n",
    "            dataset[asin][\"freq\"] = 1\n",
    "    \n",
    "    max_asin = \"\"\n",
    "    max_freq = 0\n",
    "\n",
    "    for asin, attr in dataset.items():\n",
    "        filter_list = []\n",
    "        for substitution in attr[\"also_view\"]:\n",
    "            if substitution in dataset:\n",
    "                filter_list.append(substitution)\n",
    "                dataset[substitution][\"freq\"] += 1\n",
    "\n",
    "        dataset[asin][\"also_view\"] = filter_list\n",
    "\n",
    "        filter_list = []\n",
    "        for complementary in attr[\"also_buy\"]:\n",
    "            if complementary in dataset:\n",
    "                filter_list.append(complementary)\n",
    "                dataset[complementary][\"freq\"] += 1\n",
    "\n",
    "        dataset[asin][\"also_buy\"] = filter_list\n",
    "\n",
    "        if dataset[asin][\"freq\"] > max_freq:\n",
    "            max_asin = asin\n",
    "            max_freq = dataset[asin][\"freq\"]\n",
    "\n",
    "    return max_asin\n",
    "\n",
    "\n",
    "def sample_items(dataset:dict, root, num_substituion=0, num_complements=0, picked = []):\n",
    "    \n",
    "    def _bfs_sample(dataset, root, num, relation:str, picked = picked):\n",
    "        \n",
    "        if relation not in ['also_buy', 'also_view']:\n",
    "            raise ValueError(\"Parameter of relation is an error\")\n",
    "\n",
    "        queue = []\n",
    "        if root not in picked:\n",
    "            picked.append(root)\n",
    "        queue.append(root)\n",
    "\n",
    "        while len(queue) != 0 and num > 0:\n",
    "            queue += dataset[root][relation]\n",
    "            \n",
    "            # pop first element\n",
    "            root = queue.pop(0)\n",
    "\n",
    "            if root not in picked:\n",
    "                picked.append(root)\n",
    "                num -= 1\n",
    "\n",
    "    _bfs_sample(dataset, root, num_substituion, \"also_view\", picked)\n",
    "    _bfs_sample(dataset, root, num_complements, \"also_buy\", picked)\n",
    "    return picked\n",
    "    \n",
    "    \n",
    "# print(\"asin: \" + max_freq_asin)\n",
    "# print(\"also_view: {0}\".format(dataset[max_freq_asin][\"also_view\"]))\n",
    "# print(\"also_buy: {0}\".format(dataset[max_freq_asin][\"also_buy\"]))\n",
    "# print(\"category: {0}\".format(dataset[max_freq_asin][\"category\"]))\n",
    "# print(\"price: {0}\".format(dataset[max_freq_asin][\"price\"]))\n",
    "# print(\"freq: {0}\".format(dataset[max_freq_asin][\"freq\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387dd00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B000VYIUJS', 'B000JX5JGI', 'B000R99IFC', 'B000R9BNO6', 'B0002JZP90', 'B00005IB4S', 'B00006OAQW', 'B00005LJDF', 'B00347IDP0']\n"
     ]
    }
   ],
   "source": [
    "DIR_PATH = r\"D:\\\\論文實驗\\\\data\\\\amazon\\\\\"\n",
    "FILE = r\"preprocessed_Software.csv\"\n",
    "\n",
    "dataset = read_items(DIR_PATH + FILE)\n",
    "max_freq_asin = maxFrequency(DIR_PATH + FILE) # value of frequency has been removed\n",
    "\n",
    "result = []\n",
    "sample_items(dataset=dataset, root=max_freq_asin, num_substituion=4, num_complements=4, picked=result)\n",
    "print(result)\n",
    "\n",
    "with open(DIR_PATH + \"/sample_items.csv\", \"w\") as file:\n",
    "    file.write(\"asin,also_view,also_buy,category,price\\n\")\n",
    "    for asin in result:\n",
    "        file.write(\"{0},{1},{2},{3},{4}\\n\".format(\n",
    "            asin,\n",
    "            \" \".join(dataset[asin][\"also_view\"]),\n",
    "            \" \".join(dataset[asin][\"also_buy\"]),\n",
    "            \" \".join(dataset[asin][\"category\"]),\n",
    "            dataset[asin][\"price\"]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ac9ea",
   "metadata": {},
   "source": [
    "### Randomly Generate topic of the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "313be5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B000VYIUJS,0.2741739660172769 0.25030611388184837 0.08467651398659388 0.14887861350052942 0.24196479261375142\n",
      "B000JX5JGI,0.10895959654552784 0.09723288535941442 0.18844379082179677 0.312926617706489 0.29243710956677194\n",
      "B000R99IFC,0.3614603046015271 0.011151887741604558 0.06431939485965857 0.10662583987646782 0.4564425729207419\n",
      "B000R9BNO6,0.16376215324214993 0.24234028647171896 0.21691598815438665 0.23656736522663263 0.1404142069051119\n",
      "B0002JZP90,0.09771556493775836 0.05222904104421333 0.3586889350357551 0.2877168450361969 0.20364961394607636\n",
      "B00005IB4S,0.12383550951835588 0.2513188046817213 0.2580432959398824 0.14100583996509913 0.22579654989494136\n",
      "B00006OAQW,0.16951752438025344 0.29308221348188906 0.1387880704150025 0.29530950394497796 0.10330268777787688\n",
      "B00005LJDF,0.190826673492443 0.19934771611012467 0.2780811813747074 0.05796108736321458 0.27378334165951035\n",
      "B00347IDP0,0.06478413508214481 0.14517936335870948 0.33030105299046614 0.18486982361278004 0.27486562495589956\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "def generate(numberTopics):\n",
    "    topic = [random() for t in range(numberTopics)]\n",
    "    norm = sum(topic)\n",
    "    normTopic = [t/norm for t in topic]\n",
    "    \n",
    "    return normTopic\n",
    "\n",
    "NUM_TOPIC = 5\n",
    "smaple_file = DIR_PATH + \"/sample_items.csv\"\n",
    "topic_file = DIR_PATH + \"/items_with_\" + str(NUM_TOPIC) + \"_topic.csv\"\n",
    "\n",
    "with open(smaple_file, \"r\") as file:\n",
    "    output = open(topic_file, \"w\")\n",
    "    line = file.readline()\n",
    "    for line in file.readlines():\n",
    "        asin, *_ = line.split(\",\")\n",
    "        print(asin + \",\" + \" \".join([str(t) for t in generate(NUM_TOPIC)]))\n",
    "    output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "02780483269052b0ddef6a1fd82b82d2be0dd99a863ba9a6369e5e299a35d3c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
