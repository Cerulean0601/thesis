{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2a4755",
   "metadata": {},
   "source": [
    "The [dataset](https://jmcauley.ucsd.edu/data/amazon_v2/index.html) is metadata for Magazine Subscriptions category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3922f187-f704-4399-97eb-3db8aaad7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DIR_PATH = \"\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/dirve\")\n",
    "    DIR_PATH = r\"/content/dirve/MyDrive/研究所/Data/dblp/\"\n",
    "    sys.path.append('/content/dirve/MyDrive/Colab Notebooks/package')\n",
    "else:\n",
    "    DIR_PATH = r\"D:\\\\論文實驗\\\\data\\\\dblp\\\\\"\n",
    "    sys.path.append('D:\\\\論文實驗\\\\package')\n",
    "    sys.path.append(\"D:\\\\論文實驗\\\\env\\\\Lib\\\\site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8854303-1b98-414c-a583-a70752f4a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import preprocessingText\n",
    "\n",
    "DIR_PATH = r\"D:\\\\論文實驗\\\\data\\\\amazon\\\\\"\n",
    "DATASET = r\"meta_Software.json\"\n",
    "\n",
    "def checkAttr():\n",
    "    '''\n",
    "        check whether ASIN of the item is existing.\n",
    "        \n",
    "        Result: \n",
    "            all of the items exist ASIN of the properties.\n",
    "    '''\n",
    "    with open(DIR_PATH + DATASET, \"r\") as f:\n",
    "        data = pd.read_json(f, orient=\"records\", typ=\"series\", lines=True)\n",
    "    \n",
    "        for item in data:\n",
    "            if not all(key in item for key in [\"asin\", \"also_view\", \"also_buy\", \"category\", \"price\"]):\n",
    "                print(item)\n",
    "\n",
    "def reduceAttr(output_file):\n",
    "    '''\n",
    "        保留需要的商品屬性, asin, also_view, also_buy, category, price.\n",
    "        also_view和also_buy有多個以上時, 用空白隔開\n",
    "    '''\n",
    "    \n",
    "    with open(DIR_PATH + DATASET, \"r\") as f:\n",
    "        data = pd.read_json(f, orient=\"records\", typ=\"series\", lines=True, dtype={\"category\":list})\n",
    "\n",
    "        output = open(DIR_PATH + output_file, \"w\")\n",
    "        for item in data:\n",
    "            also_view = \" \".join(item[\"also_view\"])\n",
    "            also_buy = \" \".join(item[\"also_buy\"])\n",
    "            category = \" \".join([preprocessingText(string) for string in item[\"category\"]])\n",
    "            price = item[\"price\"][1:] if item[\"price\"] != \"\" and item[\"price\"][0] == \"$\" else \"\"\n",
    "\n",
    "            if also_view and also_buy and category and price:\n",
    "                row = \"{asin},{also_view},{also_buy},{category},{price}\".format(\n",
    "                    asin = item[\"asin\"], \n",
    "                    also_view = also_view, \n",
    "                    also_buy = also_buy, \n",
    "                    category = category, \n",
    "                    price = price.replace(\",\", \"\"))\n",
    "                output.write(row + \"\\n\")\n",
    "        output.close()\n",
    "checkAttr()\n",
    "reduceAttr(\"extraction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f99070-2fa6-44f6-9899-410c600a0ddb",
   "metadata": {},
   "source": [
    "### 需要保留的商品屬性\n",
    "- asin\n",
    "- also_view\n",
    "- also_buy\n",
    "- category\n",
    "- price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793ca8f-f09d-4b68-a210-5c87a9ad58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DIR_PATH + DATASET, \"r\") as f:\n",
    "    data = pd.read_json(f, orient=\"records\", typ=\"series\", lines=True)\n",
    "    count = 0\n",
    "    for item in data:\n",
    "        if item[\"asin\"] == \"0078605407\":\n",
    "            count += 1\n",
    "            also_view = \" \".join(item[\"also_view\"])\n",
    "            also_buy = \" \".join(item[\"also_buy\"])\n",
    "            category = \" \".join([preprocessingText(string) for string in item[\"category\"]])\n",
    "            price = item[\"price\"][1:] if item[\"price\"] != \"\" and item[\"price\"][0] == \"$\" else \"\"\n",
    "            \n",
    "            print(\"{asin},{also_view},{also_buy},{category},{price}\".format(\n",
    "                    asin = item[\"asin\"], \n",
    "                    also_view = also_view, \n",
    "                    also_buy = also_buy, \n",
    "                    category = category, \n",
    "                    price = str(price)))\n",
    "#print(\"{0}/{1}\".format(count, data.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be27f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DIR_PATH + \"extraction.csv\", \"r\") as f:\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        count = count + 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c671ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_items(filename):\n",
    "    dataset = dict()\n",
    "    with open(filename) as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            asin, also_view, also_buy, price = line.split(\",\")\n",
    "            dataset[asin] = dict()\n",
    "            dataset[asin][\"also_view\"] = also_view.split(\" \")\n",
    "            dataset[asin][\"also_buy\"] = also_buy.split(\" \")\n",
    "            dataset[asin][\"price\"] = float(price)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def sample_items(dataset:dict, root, num_substituion=0, num_complements=0, picked = []):\n",
    "    \n",
    "    def _bfs_sample(dataset, root, num, relation:str, picked = picked):\n",
    "        \n",
    "        if relation not in ['also_buy', 'also_view']:\n",
    "            raise ValueError(\"Parameter of relation is an error\")\n",
    "\n",
    "        queue = []\n",
    "        if root not in picked:\n",
    "            picked.append(root)\n",
    "        queue.append(root)\n",
    "\n",
    "        while len(queue) != 0 and num > 0:\n",
    "            for i in dataset[root][relation]:\n",
    "                if i in dataset and i not in picked:\n",
    "                    queue.append(i)\n",
    "            \n",
    "            # pop first element\n",
    "            root = queue.pop(0)\n",
    "\n",
    "            if root not in picked:\n",
    "                picked.append(root)\n",
    "                num -= 1\n",
    "        return num\n",
    "\n",
    "    num_substituion = _bfs_sample(dataset, root, num_substituion, \"also_view\", picked)\n",
    "    num_complements = _bfs_sample(dataset, root, num_complements, \"also_buy\", picked)\n",
    "    return num_substituion, num_complements\n",
    "    \n",
    "    \n",
    "# print(\"asin: \" + max_freq_asin)\n",
    "# print(\"also_view: {0}\".format(dataset[max_freq_asin][\"also_view\"]))\n",
    "# print(\"also_buy: {0}\".format(dataset[max_freq_asin][\"also_buy\"]))\n",
    "# print(\"price: {0}\".format(dataset[max_freq_asin][\"price\"]))\n",
    "# print(\"freq: {0}\".format(dataset[max_freq_asin][\"freq\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "387dd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = r\".\\\\data\\\\amazon\\\\\"\n",
    "FILE = r\"preprocessed_Software.csv\"\n",
    "\n",
    "dataset = read_items(DIR_PATH + FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c23b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "num_substituion, num_complements = sample_items(dataset=dataset, root=\"B000VYIUJS\", num_substituion=9, num_complements=10, picked=result)\n",
    "\n",
    "# for asin in dataset.keys():\n",
    "#     if len(result) <= 100:\n",
    "#         num_substituion, num_complements = sample_items(dataset=dataset, root=asin, num_substituion=num_substituion, num_complements=num_complements, picked=result)\n",
    "\n",
    "# for i in result:\n",
    "#     attr = dataset[i]\n",
    "#     also_view = set(attr[\"also_view\"])\n",
    "#     also_buy = set(attr[\"also_buy\"])\n",
    "#     inter = also_buy.intersection(also_view)\n",
    "#     print(\"{}: (also_view, {}), (also_buy, {}), (inter, {})\".format(i, len(also_buy), len(also_view), len(inter)))\n",
    "\n",
    "with open(DIR_PATH + \"/sample_items.csv\", \"w\") as file:\n",
    "    file.write(\"asin,also_view,also_buy,price\\n\")\n",
    "    for asin in result:\n",
    "        also_view = [a for a in dataset[asin][\"also_view\"] if a in result]\n",
    "        also_buy = [a for a in dataset[asin][\"also_buy\"] if a in result]\n",
    "        \n",
    "        file.write(\"{},{},{},{}\\n\".format(\n",
    "            asin,\n",
    "            \" \".join(also_view),\n",
    "            \" \".join(also_buy),\n",
    "            dataset[asin][\"price\"]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ea774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e82ac9ea",
   "metadata": {},
   "source": [
    "### Randomly Generate topic of the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "313be5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "def generate(numberTopics):\n",
    "    topic = [random() for t in range(numberTopics)]\n",
    "    norm = sum(topic)\n",
    "    normTopic = [t/norm for t in topic]\n",
    "    \n",
    "    return normTopic\n",
    "\n",
    "NUM_TOPIC = 5\n",
    "DIR_PATH = r\"C:\\\\Users\\\\5F19 DS Lab\\\\Desktop\\\\HeJi\\\\論文實驗\\\\data\\\\amazon\"\n",
    "smaple_file = DIR_PATH + \"/sample_items.csv\"\n",
    "topic_file = DIR_PATH + \"/items_with_\" + str(NUM_TOPIC) + \"_topic.csv\"\n",
    "\n",
    "with open(smaple_file, \"r\") as file:\n",
    "    output = open(topic_file, \"w\")\n",
    "    line = file.readline()\n",
    "    for line in file.readlines():\n",
    "        asin, *_ = line.split(\",\")\n",
    "        output_line = asin + \",\" + \" \".join([str(t) for t in generate(NUM_TOPIC)])\n",
    "        output.write(output_line + \"\\n\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfbcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13496512853076195,\n",
       " 0.0038977556344887746,\n",
       " 0.2643477716479573,\n",
       " 0.2544791427993728,\n",
       " 0.34231020138741924]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(NUM_TOPIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "02780483269052b0ddef6a1fd82b82d2be0dd99a863ba9a6369e5e299a35d3c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
